# はじめての埋め込み（Embedding）入門

自然言語処理でいう「埋め込み」は、単語や文章といった離散的な記号の世界を、計算しやすい連続ベクトル空間へ写すことです。数学での *embedding*（構造を保ちながら別の空間へ入れ込む写像）と同じ発想で、意味や関係をできるだけ壊さずに別世界へ「埋め込む」点がポイントです。

## 埋め込みとは何か
- 単語や文などの記号を `(0.12, -0.58, 1.03, …)` のような数値ベクトルへ対応させる写像。
- 元の世界では離散的で扱いづらい対象を、256次元や768次元といったベクトル空間へ移し、計算可能な形にする。
- ベクトル空間では距離や角度が測れるため、「似ている意味ほど近い位置に並ぶ」ように学習させる。

## 単語埋め込みと文埋め込み
| 種類 | 何をベクトル化？ | 代表的な用途 |
| --- | --- | --- |
| 単語埋め込み | 「犬」「経営」「AI」など単語ごと | 類義語探索、単語レベルの意味操作（`王 − 男 + 女 ≒ 女王` など） |
| 文埋め込み | 文・段落・文書 | 文書検索、意味ベースの検索、Q&Aマッチング |

- 単語埋め込みは単語同士の関係（意味の近さ）を保つことを狙う。
- 文埋め込みは単語の並びや文脈も考慮し、文単位で「似た意味なら近いベクトル」になるように学習する。

## コサイン類似度がよく使われる理由
- 2 つのベクトルの向き（なす角）を測る指標：`cos(θ) = (A・B) / (|A||B|)`。
- ノルムで割るため長さの差を無視し、「方向＝意味の向き」に集中できる。
- 計算が単純で高速。`1` に近いほど同じ方向、`0` に近いほど関係が薄いイメージ。

## かんたんな実験アイデア
1. 元となる文と、それに関連しそうな複数の文（例：10 文）を人間の感覚で似ている順に並べてみる。
2. 同じ文を埋め込みモデルへ入力し、コサイン類似度で順位付けを行う。
3. 人間の順位とモデルの順位を比べ、差が大きい文を観察するとモデルの得意・不得意が見えてくる。

この流れを押さえれば、「なぜembeddingと呼ぶのか」「単語・文で何が違うのか」「なぜコサイン類似度が定番なのか」が直感的に理解できます。
